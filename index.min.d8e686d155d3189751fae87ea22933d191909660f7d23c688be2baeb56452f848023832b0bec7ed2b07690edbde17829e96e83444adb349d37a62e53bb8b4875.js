var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/project1/introduction/",title:"Introduction",description:"Built NLP classification models to predict helpfulness of Amazon product reviews using PySpark on Hadoop cluster in a team.",content:`Amazon Review Analysis \u0026amp; Application # Project Objective: # To help amazon optimize and improve its overall ecommerce experience through the scope of customer and seller
Project Approach: # Helpful reviews result in better sales performance, less return, and a higher conversion rate. ​​ # 👉 Predict helpful reviews (0,1) before the review receives any traction and customers' votes on its helpfulness (classification). In return, Amazon can better understand how to rank their reviews, and sellers can identify helpful reviews and use them to improve selling strategies and products.
Dataset: # Amazon Product Dataset: Amazon products across major categories with customer reviews, star ratings and helpful votes.
Original Variables # marketplace​​ customer_id​​ review_id​​ product_id ​​product_parent​ product_title ​ product_category​​ star_rating​​ helpful_votes​​ total_votes​​ vine​​ verified_purchase​​ review_headline​​ review_body​​ review_date Used Variables # review_headline​ review_id​​ review_body​​ customer_id ​ product_id ​ star_rating ​ helpful_votes ​ total_votes `}).add({id:1,href:"/docs/project2/introduction/",title:"Introduction",description:`Used Car Price Prediction # Project Objective: # Business Problem: # Asymmetric information, also known as “information failure,” takes place during a transaction where one party has greater material knowledge or better information than the other party.​
“The Market of Lemons: Quality Uncertainty and the Market Mechanism,” by George Akerlof
Consumers face incomplete​ information on used cars. The imbalanced information between sellers and buyers give more power to sellers manipulating the prices and giving buyers uncomfortable buying position.`,content:`Used Car Price Prediction # Project Objective: # Business Problem: # Asymmetric information, also known as “information failure,” takes place during a transaction where one party has greater material knowledge or better information than the other party.​
“The Market of Lemons: Quality Uncertainty and the Market Mechanism,” by George Akerlof
Consumers face incomplete​ information on used cars. The imbalanced information between sellers and buyers give more power to sellers manipulating the prices and giving buyers uncomfortable buying position. ​
Our Solution: # Help consumers stay informed on what features determine car price​ Help consumers and sellers have baseline for reasonable price​ points for cars​ Better information leads to better decisions on major purchases like cars​ Project Approach: # Project Flow: # We used dataset scraped from Craiglist and complimented the dataset with MSRP dataset. MSRP is a manufacture\u0026rsquo;s suggested listing price. Initialy intention was to look into price depreciation per car and how beginning and selling prices differ in EDA.
Caveat of using an existing dataset was it lacked recent years of dataset, so we used Scrapy, web-scraping tool, to scrape the necessary data from iSeeCars.com.
Having three datasets to combine, we used Difflib python package and used sequence matcher to match datasets by car models and manufacturers.
In A Nutshell: # As shown above, the project flow starts with data collection. Then, we perform EDA using statistical inference, clustering and topical modelling given textual columns. Then, after data preprocessing, feature engineering and feature selection, we run our selected models and improve our performance via re-engineering and hyperparameter tuning.
`}).add({id:2,href:"/docs/project3/introduction/",title:"Introduction",description:`Analysis On Beauty Industry # Our goal is to bring more meaningful and multi-dimensional data to users on cosmetic brands and products so that both brands and end-consumers can make informed decisions
Data Collection: utilize different tools such as Tweepy API, XHR requests to extract product, twitter, brands, reviews and ingredient data from various sources
Data Cleaning: utilize Python and OpenRefine to clean data and make them ready to import into MySQL`,content:`Analysis On Beauty Industry # Our goal is to bring more meaningful and multi-dimensional data to users on cosmetic brands and products so that both brands and end-consumers can make informed decisions
Data Collection: utilize different tools such as Tweepy API, XHR requests to extract product, twitter, brands, reviews and ingredient data from various sources
Data Cleaning: utilize Python and OpenRefine to clean data and make them ready to import into MySQL
Data Storage: database is stored in cloud (GCP) to allow multiple remote collaborators and MySQL Workbench was chosen as GUI tool
Data Model: EER and dimensional model created
Data Analysis: utilize SQL, Pandas, NLP libraries to perform analysis
Data Visualization On 5 Business Use Cases： Brand Perception Based On Sentiment Of Tweets Key Performance User Journey On Demo App Built On Tkinter Competitor Lookup Customer Demographic and Attribute Distribution
`}).add({id:3,href:"/docs/project2/datacollection/",title:"Data Collection",description:`Dataset # Our dataset is taken from Kaggle public dataset (1.45 GB)​. Based on resale car listings on Craigslist​, columns include selling price, car attributes, color, condition, VIN, mileage, make and model, etc.​ We enhanced and imputed some data with data from iSeeCars.
Used Car Dataset
MSRP Dataset
iSeeCar
👉 Since each listing data is 100% based on seller entry on Craiglist, there is high possibility sellers tend to input false data, resulting in datapoints that are anormal.`,content:`Dataset # Our dataset is taken from Kaggle public dataset (1.45 GB)​. Based on resale car listings on Craigslist​, columns include selling price, car attributes, color, condition, VIN, mileage, make and model, etc.​ We enhanced and imputed some data with data from iSeeCars.
Used Car Dataset
MSRP Dataset
iSeeCar
👉 Since each listing data is 100% based on seller entry on Craiglist, there is high possibility sellers tend to input false data, resulting in datapoints that are anormal. Incorporating External Data # To impute some of the columns, we looked at the most common​ value for each car model​.
Assumptions:​
Cars of the same make and model share common attributes ​such as cylinders, type and size.​ For customizable attributes (drive type) we are assuming the most common values for each model​. We collected data from iSeeCars through web scraping​. Data scraped include MSRP and car attributes for all makes and​ models available.
Problem:​ # Due to user-input values, car model names may not match​. exactly with the clean names from iSeeCars​. To overcome this problem, we used OpenRefine to try and fix some of the entries based on naming clusters​. We also used SequenceMatcher from the difflib library to​ programmatically fix model names based on similarity index​ and assigned model with the highest name similarity. Example of cases where difflib sequence matcher was needed:
Available Columns And Relevant Columns # After combining data and cleaning it up, we looked in each column. We determined the relevant columns by looking at distribution of data, correlation with price, missing values and other factors from inference.
Irrelevant # High Cardinality
ID URL Image_URL VIN Categorical
Region Region_URL Null
County(100% missing) Size (\u0026gt;70%) Geolocation:
Longitude Latitude Text
Description Relevant # Categorical
Manufacturer​ Model ​ Condition​ Cylinders​ Fuel​ Title Status​ Transmission​ Drive​ Type​ Paint Color​ State Continuous
Price MSRP Odometer Time
Year Posting Date Initial Feature Selection And Imputation # Dropped Columns # ID, VIN, URL, Lattitude, Longitude, Image URL, Region, Region URL, County, Size, Description
Imputed Columns # Mode:
Title Status, Fuel, Color
Based on other features:
Condition
Based on external dataset:
Manufacturer, Model, Drive, Cylinder, Type
`}).add({id:4,href:"/docs/project3/data-profile/",title:"Data Profile",description:"Data Infrastructure # Data Profile # SOURCE DESCRIPTION DATA SIZE FORMAT Product Kaggle Products listed on Sephora 12.4MB csv Brands Phantom Buster Information on cosmetic brands and company information 226KB json/csv Reviews BazaarVoice API User product reviews on Sephora 500MB json Twitter Tweepy API Users’ tweets with hashtag of brand name 7.3MB csv Ingredient California Safe Cosmetics Program Product DB Chemicals used in cosmetics that are known to be harmful 2.",content:`Data Infrastructure # Data Profile # SOURCE DESCRIPTION DATA SIZE FORMAT Product Kaggle Products listed on Sephora 12.4MB csv Brands Phantom Buster Information on cosmetic brands and company information 226KB json/csv Reviews BazaarVoice API User product reviews on Sephora 500MB json Twitter Tweepy API Users’ tweets with hashtag of brand name 7.3MB csv Ingredient California Safe Cosmetics Program Product DB Chemicals used in cosmetics that are known to be harmful 2.2MB csv Data Quality Dimensions Assessment # Completeness Accuracy Consistency Validity Uniqueness Integrity Original data did not contain all products up to this year and tweets data was limited to recent data Tweet data contained inaccurate feedback on brands as search query was simply limited to \u0026ldquo;#brand” and not advanced keywords such as product names The amount of data for product, reviews and tweets per brand was not consistent For ingredient harmfulness, the data is perfectly valid as it has scientific backing. Using tweet sentiment and review data however, may not be perfectly representative of reality A customer may have repeated review based on multiple experience Sentiment analysis may be based on multiple tweets that could have been one Entity, domain and referential integrities were considered with constraints ETL Process: Extraction, Transformation \u0026amp; Load # Implementation Tools # Design Consideraton # Branding Original product contains products across all categories, but we limit our data to facial brands under Sephora’s makeup and skincare categories.
Outliers \u0026amp; Anomalies Tweet location data contains non-location data. Intensive clustering and replacing with NULL if no-location can be derived from.
Ingredients Intermediary lookup table linking harmful ingredients to the product’ ingredient. Dealing With NAs MySQL Workbench recognizes NULL instead of empty string so removed unused columns and filled NaNs as NULL.
Product Reviews The reviews have too many attributes that are sparsely populated due to the difference between product categories, making it hard to create table columns in SQL for it.
Thus, using NoSQL and store as key value pairs is what we decided to do.
Data Naming Convention Standardize the format for names across entities and attributes (snake case).
Normalization Using brand id across the tables, referring to the brand table to normalize.
`}).add({id:5,href:"/docs/project1/eda/",title:"EDA",description:"Explorative Data Analysis",content:`Initial General EDA # General EDA Top2Vec # Intention of running topical modelling on review column of dataset is to check whether it holds sufficient information for us to move forward with our project, and the breadth and depth of information in reviews is hard to capture manually given large dataset.
Import Spark NLP and import Top2Vec # #Spark NLP import sparknlp from sparknlp.pretrained import PretrainedPipeline from sparknlp.annotator import * from sparknlp.base import * from pyspark.sql import SparkSession from pyspark.sql.functions import * import pyspark.sql.functions as F #Top2Vec from top2vec import Top2Vec Initialize Spark NLP context # # Initialize spark context spark = SparkSession.builder \\ .appName(\u0026quot;Spark NLP\u0026quot;)\\ .master(\u0026quot;local[4]\u0026quot;)\\ .config(\u0026quot;spark.driver.memory\u0026quot;,\u0026quot;16G\u0026quot;)\\ .config(\u0026quot;spark.driver.maxResultSize\u0026quot;, \u0026quot;0\u0026quot;) \\ .config(\u0026quot;spark.kryoserializer.buffer.max\u0026quot;, \u0026quot;2000M\u0026quot;)\\ .config(\u0026quot;spark.jars.packages\u0026quot;, \u0026quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.1\u0026quot;)\\ .getOrCreate() Read the dataset with spark # # Read data df = spark.read \\ .option(\u0026quot;quote\u0026quot;, \u0026quot;\\\u0026quot;\u0026quot;) \\ .option(\u0026quot;escape\u0026quot;, \u0026quot;\\\u0026quot;\u0026quot;) \\ .option(\u0026quot;ignoreLeadingWhiteSpace\u0026quot;,True) \\ .csv(\u0026quot;dataset.csv\u0026quot;,inferSchema=True,header=True, sep = ',') Create a NLP pipeline that cleans the review_text, removing html tags and unnecessary words # documentAssembler = DocumentAssembler() \\ .setInputCol('review_text') \\ .setOutputCol('document') cleanUpPatterns = [\u0026quot;\u0026lt;[^\u0026gt;]*\u0026gt;\u0026quot;] documentNormalizer = DocumentNormalizer() \\ .setInputCols(\u0026quot;document\u0026quot;) \\ .setOutputCol(\u0026quot;normalizedDocument\u0026quot;) \\ .setAction(\u0026quot;clean\u0026quot;) \\ .setPatterns(cleanUpPatterns) \\ .setReplacement(\u0026quot; \u0026quot;) \\ .setPolicy(\u0026quot;pretty_all\u0026quot;) \\ .setLowercase(True) sentenceDetector = SentenceDetector() \\ .setInputCols([\u0026quot;normalizedDocument\u0026quot;]) \\ .setOutputCol(\u0026quot;sentence\u0026quot;) tokenizer = RegexTokenizer() \\ .setInputCols(['sentence']) \\ .setOutputCol('token') \\ .setPattern(\u0026quot;\\\\W\u0026quot;) \\ .setToLowercase(True) stopwords_cleaner = StopWordsCleaner()\\ .setInputCols(['token']) \\ .setOutputCol('clean') \\ .setCaseSensitive(False) finisher = Finisher() \\ .setInputCols([\u0026quot;clean\u0026quot;]) \\ docPatternRemoverPipeline = \\ Pipeline() \\ .setStages([ documentAssembler, documentNormalizer, sentenceDetector, tokenizer, stopwords_cleaner, finisher]) ds = docPatternRemoverPipeline.fit(df).transform(df) Transform cleaned dataset into Top2Vec input format # #Convert finished_clean column into string from array\u0026lt;string\u0026gt; ds2 = ds.withColumn('finished_clean', concat_ws(',', 'finished_clean')) #Remove comma from flattened string finished_clean ds2 = ds2.withColumn(\u0026quot;finished_clean\u0026quot;, F.regexp_replace('finished_clean',r'[,]',' ')) #Final clean review data ds2 = ds2.select('finished_clean') #FlatMap to put it into Top2Vec model review_text=ds2.rdd.flatMap(lambda x: x).collect() Universal Sentence Encoder is used for Top2Vec # Bert is also an option, but I chose USE because it is one of the latest powerful Spark NLP transformer.
#Top2Vec powered by universal-sentence-encoder is going to analyze preprocessed review_text data and cluster them into topics with keywords model=Top2Vec(documents=review_text, embedding_model='universal-sentence-encoder') #Print total number of topics model.get_num_topics() #Extract necessary information from the trained model topic_words, word_scores, topic_nums = model.get_topics(191) #Print out topic wordclouds for topic in topic_nums[1:30]: model.generate_topic_wordcloud(topic, background_color=\u0026quot;black\u0026quot;) ​​ Insights # I could get 191 clearly defined and differentiated topic clusters from running Top2Vec on digital video category dataset, which indicates that the review text holds meaningful information.
Top2Vec Star Ratings \u0026amp; Helpful Votes Defined # Amazon as of today​ # Amazon Star Ratings Star ratings \u0026amp; helpful votes significantly contribute to making top reviews, and most customers rely on reviews to purchase products​. There can be many reasons why review is helpful. Sellers might have hard time figuring out what reviews are helpful but it is essential to increase sales​. Hence, build an NLP model to classify reviews into helpful(1) or not(0) based on given information: star ratings, review text​
`}).add({id:6,href:"/docs/project2/eda/",title:"EDA",description:`Explorative Data Analysis # Perforing EDA, we found a couple of interesting insights.
Price against key features Price against several key variables:​ # There seems to be a significant difference of price ranges between car manufacturers. However the range within each manufacturer seems very large. It is most possibly due to high number of different car models. That\u0026rsquo;s why we will not include manufacturer and model as predictors as they can be better represented by other more relevant variables.`,content:`Explorative Data Analysis # Perforing EDA, we found a couple of interesting insights.
Price against key features Price against several key variables:​ # There seems to be a significant difference of price ranges between car manufacturers. However the range within each manufacturer seems very large. It is most possibly due to high number of different car models. That\u0026rsquo;s why we will not include manufacturer and model as predictors as they can be better represented by other more relevant variables.​ For MSRP and Production, at a glance, there doesn’t seem to be a clear linear relationship to price. But when we look closely, there\u0026rsquo;s an indication of higher price with a higher MSRP and the latest the production year is.​ Locational Insights # Location Insights ​Next we\u0026rsquo;re looking at regional distribution of the listings. From the longitude and latitude we can see that the listings are focused in the US, with some anomalies outside the US. We were not doing any transformation to this as we will not use this as predictors.​
​By state, the data is distributed quite evenly. We can also sense that there is a pattern that certain states have narrower range than other states. So based on this, state may be a good predictor for the price.
Odometer # Odometer We would like to give special highlight to odometer. As you can see, the raw data is highly right-skewed, indicating there are high anomalies and outliers.​
We tried to normalize the data by creating decision threshold based on the distribution and literary findings (would be explained in more details during feature engineering part). And we can see that there is a clearer relationship to price after we did the cleanup.
Other Features # Other Features For other variables, we were looking at the distribution of each categories.​
There seems to be a clear dominance of certain categories in certain variables.​
We used this as one of the imputation methods where we would pick the mode to fill up the nulls, when there is no better approach.​
Other possible method is to reduce the possible skewness and clearer distinction by imposing decision threshold to make the categories binary, such as for paint color.​
Insights From Clustering # ​ Clustering We also tried to perform clustering to understand the data better in terms of the market segment as one of our problem question.​
We experimented with multiple clustering methods, and we decided to choose K-Means with 3 clusters that makes the most sense to interpret.​
Cluster Price Odometer MSRP Car Age Vintage Color Cluster0 \u0026ldquo;Lower\u0026rdquo; Min: 1k Max: 52k Mean: 7k Min: 143k Max: 268k Mean: 177k Max: 65k Mean: 30k Mean: 14 0.04% 46% neutral Cluster1 \u0026ldquo;Mid\u0026rdquo; Min: 1k Max: 80k Mean: 12k Min: 75k Max: 125k Mean: 110k Max: 96k Mean: 31k Mean: 10 0.30% 51% neutral Cluster2 \u0026ldquo;Upper\u0026rdquo; Min: 1k Max: 90k Mean: 21k Min: 0 Max: 85k Mean: 45k Max: 102k Mean: 33k Mean: 6 0.82% 53% neutral We visualized all of the independent variables against price, and these three variables seem to have the clearest distinction and indicate that they might good predictors.​
We have 3 clusters:​
Cluster 0, as the \u0026ldquo;Lower Class Segment\u0026quot;​
Cluster 1, as the \u0026ldquo;Middle Class Segment\u0026quot;​
Cluster 2, as the \u0026ldquo;Upper Class Segment\u0026quot;​
We can see that Price and MSRP will consistently increase following the higher the class is, while Odometer and Car Age will decrease as the class gets higher. Interestingly, the proportion of vintage and neutral cars increase in higher segments.​
We use all of these variables to test out the model.
Code Reference
EDA
EDA2
EDA Visalizations
Top2Vec On Car Description # For the \u0026ldquo;Description\u0026rdquo; column of our data, we use Top2Vec algorithm to see whether we can extract useful information​
Top2Vec \u0026ndash; An algorithm that can perform topic modeling on text. It returns the number of the topics it finds from the text, and the keywords from each topic. It can also generate word cloud to help us visualize the keywords in certain topic.
From our result, we get more than 1000 topics back, and the keywords in each topic are not similar with each other. Thus, the “Description” column is not informational and should not be included in our model​
Top2Vec Topical Modelling Result However, one insight we have is that a lot of used car company advertise their car on craigslist since a lot of the keywords revolve the names of some used car companies, like Carfax, Carmax, Autotrader, etc.
This gave us confidence that we can drop description feature for modelling.
EDA Using Top2Vec
`}).add({id:7,href:"/docs/project3/data-collection/",title:"Data Collection",description:`XHR # XHR provided a better solution to scrape Sephora product reviews over BS4 \u0026amp; Selenium
Since scraping with both BS4 and Selenium did not work, we turned to XHR requests This turned to our advantage since the XHR API was generally cleaner, more complete product listing (website removed old products) and contains reviewer metadata
Tweepy API # Tweeter developer account created to use the free-tier api calls to scrape tweet data per brand (brand, tweet_id, tweet_date, follower_count, retweets, location, tweet_text) Iterate over brand names in brands table and append collected tweets to dataframe using Pandas For 186 brands, avg 110 tweets per brand were collected but number of tweets varied per brand, which could result in diminishing the value of analysis Phantom Buster # Phantom Buster was a great substitute to BS4 to scrape brand data.`,content:`XHR # XHR provided a better solution to scrape Sephora product reviews over BS4 \u0026amp; Selenium
Since scraping with both BS4 and Selenium did not work, we turned to XHR requests This turned to our advantage since the XHR API was generally cleaner, more complete product listing (website removed old products) and contains reviewer metadata
Tweepy API # Tweeter developer account created to use the free-tier api calls to scrape tweet data per brand (brand, tweet_id, tweet_date, follower_count, retweets, location, tweet_text) Iterate over brand names in brands table and append collected tweets to dataframe using Pandas For 186 brands, avg 110 tweets per brand were collected but number of tweets varied per brand, which could result in diminishing the value of analysis Phantom Buster # Phantom Buster was a great substitute to BS4 to scrape brand data.
Attempted to scrape data from Wikipedia, but not all brands are listed on Wikipedia Discovered most brands have their company page on LinkedIn providing valuable information Chose Phantom Buster to extract brand information automatically from LinkedIn The extraction result is in JSON/CSV format `}).add({id:8,href:"/docs/project1/dataprep/",title:"Data Preparation",description:`Data Preparation # Remove NaN
Get Products With Total Votes \u0026gt; 10
df = df.filter(col('total_votes') \u0026gt; 10) df.count() Combine review_headline with review_body to create review text By doing so, we can minimize information loss by reducing number of null rows by combining both columns, as they have different number of null rows. df = df.fillna(\u0026quot;\u0026quot;, \u0026quot;review_body\u0026quot;) df = df.fillna(\u0026quot;\u0026quot;, \u0026quot;review_headline\u0026quot;) df = df.withColumn('review_text', F.concat('review_headline', F.lit(\u0026quot; \u0026quot;), 'review_body')) df.show(1, vertical = True, truncate = False) Create Helpful Ratio \u0026amp; Helful column for our modelling Helpful Ratio: helpful_votes/ total votes ​ Created Column Helpful: ​`,content:`Data Preparation # Remove NaN
Get Products With Total Votes \u0026gt; 10
df = df.filter(col('total_votes') \u0026gt; 10) df.count() Combine review_headline with review_body to create review text By doing so, we can minimize information loss by reducing number of null rows by combining both columns, as they have different number of null rows. df = df.fillna(\u0026quot;\u0026quot;, \u0026quot;review_body\u0026quot;) df = df.fillna(\u0026quot;\u0026quot;, \u0026quot;review_headline\u0026quot;) df = df.withColumn('review_text', F.concat('review_headline', F.lit(\u0026quot; \u0026quot;), 'review_body')) df.show(1, vertical = True, truncate = False) Create Helpful Ratio \u0026amp; Helful column for our modelling Helpful Ratio: helpful_votes/ total votes ​ Created Column Helpful: ​
helpful_ratio \u0026lt; 0.5: not helpful = 0 ​
helpful_ratio \u0026gt; 0.5: helpful = 1
df = df.withColumn('helpful_ratio', F.col('helpful_votes') / F.col('total_votes')) df = df.withColumn('helpful', when(col(\u0026quot;helpful_ratio\u0026quot;) \u0026lt; 0.5, 0).otherwise(1)) Further cleanup on text using REGEX operation from pyspark.sql.functions import col, lower, regexp_replace, split def clean_text(c): c = lower(c) c = regexp_replace(c, \u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) c = regexp_replace(c, \u0026quot;^rt \u0026quot;, \u0026quot;\u0026quot;) c = regexp_replace(c, \u0026quot;(https?\\://)\\S+\u0026quot;, \u0026quot;\u0026quot;) c = regexp_replace(c, \u0026quot;\u0026lt;.*?\u0026gt;|\u0026amp;([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\u0026quot;, \u0026quot;\u0026quot;) return c df_clean = df_clean.withColumn('review_text',clean_text(col(\u0026quot;review_text\u0026quot;)).alias(\u0026quot;text\u0026quot;)) `}).add({id:9,href:"/docs/project3/data-preparation/",title:"Data Preparation",description:`Data Cleanup # Remove NaN Remove Emojis from tweets REGEX to clean tweet text REGEX: Tweet text data includes emoji, hashtag, numbers, links and etc so REGEX is used to clean up each text
NLTK: NLTK tokenizing by word was used to extract more meaningful and logical words from cleaned tweet texts
Clustering On Location Data From Tweets # OpenRefine Tweets’ location is not structured so cleaning up the users’ various input on location required heavy clustering that involved:`,content:`Data Cleanup # Remove NaN Remove Emojis from tweets REGEX to clean tweet text REGEX: Tweet text data includes emoji, hashtag, numbers, links and etc so REGEX is used to clean up each text
NLTK: NLTK tokenizing by word was used to extract more meaningful and logical words from cleaned tweet texts
Clustering On Location Data From Tweets # OpenRefine Tweets’ location is not structured so cleaning up the users’ various input on location required heavy clustering that involved:
Key Collision Methods Nearest Neighbor Methods Fingerprinting N-Gram Fingerprint Phonetic Fingerprint (metaphone3 was the most effective in finding and clustering data that was not appropriate as location) For data that was not classified as location, “ ” was assigned.
`}).add({id:10,href:"/docs/project1/modelling/",title:"Modelling",description:"Recap # Amazon reviews that are voted as helpful are assumed to be reviews that people will naturally want to read more of​ Reviews are an important part of the consumer\u0026rsquo;s purchasing decisions. They are just as, if not more important than star ratings that are not descriptive enough​ However, reviews may not have enough helpfulness score if it is new and has yet to gain any traction. Lack of participation and feedback from other purchasers can also lead to insufficient scoring​ Therefore, prediction of review helpfulness before other users even see them can help Amazon push these reviews up and provide a better consumer buying experience​ Classification problems = classifier models; Text data = natural language processing",content:`Recap # Amazon reviews that are voted as helpful are assumed to be reviews that people will naturally want to read more of​ Reviews are an important part of the consumer\u0026rsquo;s purchasing decisions. They are just as, if not more important than star ratings that are not descriptive enough​ However, reviews may not have enough helpfulness score if it is new and has yet to gain any traction. Lack of participation and feedback from other purchasers can also lead to insufficient scoring​ Therefore, prediction of review helpfulness before other users even see them can help Amazon push these reviews up and provide a better consumer buying experience​ Classification problems = classifier models; Text data = natural language processing
NLP Pipeline # Using PySpark ML NLP Pipeline # First, process review text with the following pipeline. # #Document \u0026amp; Tokenize document_assembler = DocumentAssembler().setInputCol(\u0026quot;review_text_l\u0026quot;).setOutputCol(\u0026quot;document\u0026quot;) tokenizer = Tokenizer().setInputCols([\u0026quot;document\u0026quot;]).setOutputCol(\u0026quot;review_words\u0026quot;) #Cleaning Tokens remover = StopWordsCleaner().setInputCols(\u0026quot;review_words\u0026quot;).setOutputCol(\u0026quot;review_words_stop\u0026quot;).setCaseSensitive(False).setStopWords(eng_stopwords) stemmer = Stemmer().setInputCols([\u0026quot;review_words_stop\u0026quot;]).setOutputCol(\u0026quot;review_words_lemstem\u0026quot;) finisher = Finisher().setInputCols([\u0026quot;review_words_lemstem\u0026quot;]).setOutputCols([\u0026quot;token_features\u0026quot;]).setOutputAsArray(True).setCleanAnnotations(False) pipeline_stem = Pipeline(stages=[document_assembler,tokenizer,remover,stemmer,finisher]) df_clean_nlp = pipeline_stem.fit(df_clean).transform(df_clean) hashingTF = HashingTF(inputCol=\u0026quot;token_features\u0026quot;, outputCol=\u0026quot;rawFeatures\u0026quot;, numFeatures = 10000) df_featurizedData = hashingTF.transform(df_clean_nlp) idf = IDF(inputCol=\u0026quot;rawFeatures\u0026quot;, outputCol=\u0026quot;features\u0026quot;) df_nlp = idf.fit(df_featurizedData).transform(df_featurizedData) Second, perform Logistic Regression \u0026amp; Random Forest Classifier on processed, vectorized dataset. # lr = LogisticRegression(featuresCol = 'features', labelCol='helpful') paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.0]).addGrid(lr.elasticNetParam, [0.0]).addGrid(lr.maxIter, [20, 100]).build()) #Evaluator evaluator = MulticlassClassificationEvaluator(labelCol='helpful', predictionCol=\u0026quot;prediction\u0026quot;) # Create 3-fold CrossValidator cv = CrossValidator(estimator=lr,estimatorParamMaps=paramGrid,evaluator=evaluator,numFolds=3) cvModel = cv.fit(train) predictions = cvModel.transform(train) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;accuracy\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;f1\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;weightedPrecision\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;weightedRecall\u0026quot;})) rfc = RandomForestClassifier(impurity=\u0026quot;gini\u0026quot;, featuresCol='features', labelCol=\u0026quot;helpful\u0026quot;) paramGrid = (ParamGridBuilder().addGrid(rfc.impurity, ['gini', 'entropy']).addGrid(rfc.maxBins, [32, 100]).build()) #Evaluator evaluator = MulticlassClassificationEvaluator(labelCol='helpful', predictionCol=\u0026quot;prediction\u0026quot;) # Create 3-fold CrossValidator cv = CrossValidator(estimator=rfc,estimatorParamMaps=paramGrid,evaluator=evaluator,numFolds=3) cvModel = cv.fit(train) predictions = cvModel.transform(train) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;accuracy\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;f1\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;weightedPrecision\u0026quot;})) print(evaluator.evaluate(predictions, {evaluator.metricName: \u0026quot;weightedRecall\u0026quot;})) Using SparkNLP Pipeline With Sentence Encoders # First, process review text with the following pipeline. # Universal Sentence Encoder # #USE (Universal Sentence Encoder) Sentence Embedding document = DocumentAssembler()\\ .setInputCol(\u0026quot;review_text\u0026quot;)\\ .setOutputCol(\u0026quot;document\u0026quot;) embeddingsSentence = UniversalSentenceEncoder.load('tfhub_use_en_2.4.0_2.4_1587136330099') \\ .setInputCols([\u0026quot;document\u0026quot;])\\ .setOutputCol(\u0026quot;sentence_embeddings\u0026quot;) classifierdl = ClassifierDLApproach()\\ .setInputCols([\u0026quot;sentence_embeddings\u0026quot;])\\ .setOutputCol(\u0026quot;prediction\u0026quot;)\\ .setLabelColumn(\u0026quot;helpful\u0026quot;)\\ .setMaxEpochs(5)\\ .setEnableOutputLogs(True) use_clf_pipeline = Pipeline( stages = [ document, embeddingsSentence, classifierdl ]) from sklearn.metrics import classification_report, accuracy_score metrics_final = metrics.toPandas() metrics_final['result'] = metrics_final['result'].apply(lambda x: x[0]) metrics_final['result'] = metrics_final['result'].astype('int') print(classification_report(metrics_final.helpful, metrics_final.result)) print(accuracy_score(metrics_final.helpful, metrics_final.result)) Uncased BERT # #BERT Sentence Embedding document = DocumentAssembler()\\ .setInputCol(\u0026quot;review_text\u0026quot;)\\ .setOutputCol(\u0026quot;document\u0026quot;) bert_cmlm = BertSentenceEmbeddings.load('sent_bert_base_uncased_en_2.6.0_2.4_1598346203624')\\ .setInputCols([\u0026quot;document\u0026quot;])\\ .setOutputCol(\u0026quot;sentence_embeddings\u0026quot;) classifierdl = ClassifierDLApproach()\\ .setInputCols([\u0026quot;sentence_embeddings\u0026quot;])\\ .setOutputCol(\u0026quot;prediction\u0026quot;)\\ .setLabelColumn(\u0026quot;helpful\u0026quot;)\\ .setMaxEpochs(5)\\ .setEnableOutputLogs(True) use_clf_pipeline = Pipeline( stages = [ document, embeddingsSentence, classifierdl ]) from sklearn.metrics import classification_report, accuracy_score metrics_final = metrics.toPandas() metrics_final['result'] = metrics_final['result'].apply(lambda x: x[0]) metrics_final['result'] = metrics_final['result'].astype('int') print(classification_report(metrics_final.helpful, metrics_final.result)) print(accuracy_score(metrics_final.helpful, metrics_final.result)) `}).add({id:11,href:"/docs/project2/pre-modelling/",title:"Pre-modelling",description:"Pre-modelling # Initial Outlier handling \u0026amp; filling in null values​ # Odometer: ​ # Highly right skewed ​ Outliers are removed: Upper threshold +7* stdev​ Condition: # Fill nulls based on odometer​ Newer condition has low odometer. Quantile \u0026lt;25% = Excellent​ Quantile 25%-50% = Good​ Quantile \u0026gt;50% = Fair​ NA = Fair​ Transmission: ​ # drop NA rows (~0.6%)​ Categorical variables: # Fill in NA with mode​ For cylinders, drive, type, fuel​ Fill NA with the most common types based on matched model and​ manufacturer​ Fill the rest of NA with mode​ Created Features: ​ # Color: 12 colors into binary column of \u0026ldquo;is_neutral\u0026quot;​",content:`Pre-modelling # Initial Outlier handling \u0026amp; filling in null values​ # Odometer: ​ # Highly right skewed ​ Outliers are removed: Upper threshold +7* stdev​ Condition: # Fill nulls based on odometer​ Newer condition has low odometer. Quantile \u0026lt;25% = Excellent​ Quantile 25%-50% = Good​ Quantile \u0026gt;50% = Fair​ NA = Fair​ Transmission: ​ # drop NA rows (~0.6%)​ Categorical variables: # Fill in NA with mode​ For cylinders, drive, type, fuel​ Fill NA with the most common types based on matched model and​ manufacturer​ Fill the rest of NA with mode​ Created Features: ​ # Color: 12 colors into binary column of \u0026ldquo;is_neutral\u0026quot;​
is_neutral (1): Black, White, Silver, Grey​ Is_neutral (0): Colorful​ car_age :
year of posting_date subtracted by year when car came out ​ is_vintage: car_age \u0026gt;50 ​ To account for vintage cars\u0026rsquo; higher price due to rarity and originality ​ Then, we changed data type into numeric format​ usubg label encoder. ​
​
`}).add({id:12,href:"/docs/project3/data-diagrams/",title:"Data Diagrams",description:"OLTP: EER Diagram # There are 8 tables in the database After data collection and processing, import our data into MySQL Workbench and perform reverse enginnering to create the schema Define attributes with approriate data type Follow standard naming convention for tables and attributes Each table contains its unique key as primary key OLAP: Dimensional Diagram # Dimensional Model was developed based on the EER model constructed Used Snowflake Schema and normalized some dimension tables such as location table and product table “Reviews” is the fact table and there are 4 entities: customers, brands, product and date An extra entity, “dim_customers”, exists in dimensional diagram.",content:`OLTP: EER Diagram # There are 8 tables in the database After data collection and processing, import our data into MySQL Workbench and perform reverse enginnering to create the schema Define attributes with approriate data type Follow standard naming convention for tables and attributes Each table contains its unique key as primary key OLAP: Dimensional Diagram # Dimensional Model was developed based on the EER model constructed Used Snowflake Schema and normalized some dimension tables such as location table and product table “Reviews” is the fact table and there are 4 entities: customers, brands, product and date An extra entity, “dim_customers”, exists in dimensional diagram. Original data source included customer data such as customer username and personal info, but due to data privacy concerns, we decided not to include customer data in our project database MongoDB # Review Context \u0026amp; Metadata Stored
MongoDB is used to store review context data This includes metadata on customer attributes such as their eye color, skin tone, if they are employees, etc Nested data fit for document data type
MongoDB is chosen because of their non-relational nature and compatibility with nested JSON Different products may have different attributes that may not make sense if put on other product categories. For example, skin type for lipstick Neo4j # Shows all the ingredients contained in the product If the ingredient has hazard traits, the node of this ingredient will be red. Choose an ingredient （or hazard ingredient) and we can see the products that contain it Graph1: (a:Ingredient)-[:IS_IN]-\u0026gt;(b:PRODUCT)
Graph2: (p:PRODUCT)-[:Has_Hazard_trait]-\u0026gt;(h:Hazard_Ingredient)
`}).add({id:13,href:"/docs/project2/re-engineering/",title:"Re-engineering",description:"Post-modelling # Re-engineering # After running the base models with initially feature engineered dataset, we saw low performing results and then re-engineered.\nContextual Anomaly Detection # Odometer: ​ # Standard car odometer should have max 300,000 miles.​ Trimming data with 75th quantile + 3*IQR, ~268,000, as a cut reduced skewness from 3.04 to 0.3​. Only new car should have 0 odometer so trimmed otherwise​. Price:​ # Dropped cars below $1000 and greater than $200,000 that​ are in the extreme ranges not fit for our analysis purpose​ Max car price was three billion dollars ​ MSRP:​ # MSRP is the manufacturer\u0026rsquo;s suggested retail price (list price) ​ Dropped MSRP \u0026lt; car price as MSRP should be higher than​ used car selling price​ Code Reference",content:`Post-modelling # Re-engineering # After running the base models with initially feature engineered dataset, we saw low performing results and then re-engineered.
Contextual Anomaly Detection # Odometer: ​ # Standard car odometer should have max 300,000 miles.​ Trimming data with 75th quantile + 3*IQR, ~268,000, as a cut reduced skewness from 3.04 to 0.3​. Only new car should have 0 odometer so trimmed otherwise​. Price:​ # Dropped cars below $1000 and greater than $200,000 that​ are in the extreme ranges not fit for our analysis purpose​ Max car price was three billion dollars ​ MSRP:​ # MSRP is the manufacturer\u0026rsquo;s suggested retail price (list price) ​ Dropped MSRP \u0026lt; car price as MSRP should be higher than​ used car selling price​ Code Reference
Re-engineering​
`}).add({id:14,href:"/docs/project1/result/",title:"Result",description:"Result of Logistic Regression \u0026amp; Random Forest # Product Category Logistic Regression Random Forest Books 0.85 / 0.84 0.83 / 0.83 E-Books 0.98 / 0.71 0.77 / 0.78 Music 0.85 / 0.83 0.72 / 0.72 Digital Music 0.82 / 0.74 0.67 / 0.68 DVD 0.80 / 0.78 0.68 / 0.68 Digital Video 0.78 / 0.72 0.66 / 0.65 Software 0.94 / 0.82 0.86 / 0.86 Digital Software 0.99 / 0.84 0.",content:`Result of Logistic Regression \u0026amp; Random Forest # Product Category Logistic Regression Random Forest Books 0.85 / 0.84 0.83 / 0.83 E-Books 0.98 / 0.71 0.77 / 0.78 Music 0.85 / 0.83 0.72 / 0.72 Digital Music 0.82 / 0.74 0.67 / 0.68 DVD 0.80 / 0.78 0.68 / 0.68 Digital Video 0.78 / 0.72 0.66 / 0.65 Software 0.94 / 0.82 0.86 / 0.86 Digital Software 0.99 / 0.84 0.82 / 0.83 Toys 0.99 / 0.84 0.89 / 0.89 Digital Video Games 0.88 / 0.70 0.69 / 0.69 Average 0.89 / 0.72 0.83 / 0.76 Result of USE # Results of running model on Digital Video category dataset
precision recall f1-score support 0 0.78 0.79 0.78 6812 1 0.75 0.75 0.75 5924 accuracy 0.77 12736 macro avg 0.77 0.77 0.77 12736 weighted avg 0.77 0.77 0.77 12736 light_model = LightPipeline(pipeline2) #Using a review that was stated Helpful on Amazon text=\u0026quot;The show is smart and awkwardly, yet deliciously, inappropriate. Miss, miss, miss Steve Carell but after a weak season 8, the Office has rebounded with season 9 and will end its run with high marks. Season 8 had its moments but the show seemed rudderless without Michael – Robert California and Nellie were just weird and Andy is no Michael. When it seemed all hope was lost, the show shifts to a more ensemble – no superstar- approach in season 9 which, with Michael gone, really works. With such wonderful characters in Dwight, Jim, Meridith, Stanley, Angela, Kevin, Oscar, Darrell and Phyllis it’s nice to have all the story lines going at once – Nellie fits in much better this year too. Andy and Erin are fine in the mix but are much better in doses than in being the main focus. A little of Andy goes a long way. That shift was a game changer in a good way.\u0026quot; light_model.annotate(text)['prediction'][0] Result: 1
#Using a review that has not beed stated Helpful on Amazon YET text=\u0026quot;Liked it\u0026quot; light_model.annotate(text)['prediction'][0] Result: 0
text=\u0026quot;I tossed it in the trash. It smelled so bad.\u0026quot; light_model.annotate(text)['prediction'][0] Result: 0
Result of BERT # Results of running model on Digital Video category dataset
precision recall f1-score support 0 0.75 0.85 0.80 6812 1 0.79 0.68 0.73 5924 accuracy 0.77 12736 macro avg 0.77 0.76 0.76 12736 weighted avg 0.77 0.77 0.77 12736 light_model = LightPipeline(pipeline2) #Using a review that was stated Helpful on Amazon text=\u0026quot;The show is smart and awkwardly, yet deliciously, inappropriate. Miss, miss, miss Steve Carell but after a weak season 8, the Office has rebounded with season 9 and will end its run with high marks. Season 8 had its moments but the show seemed rudderless without Michael – Robert California and Nellie were just weird and Andy is no Michael. When it seemed all hope was lost, the show shifts to a more ensemble – no superstar- approach in season 9 which, with Michael gone, really works. With such wonderful characters in Dwight, Jim, Meridith, Stanley, Angela, Kevin, Oscar, Darrell and Phyllis it’s nice to have all the story lines going at once – Nellie fits in much better this year too. Andy and Erin are fine in the mix but are much better in doses than in being the main focus. A little of Andy goes a long way. That shift was a game changer in a good way.\u0026quot; light_model.annotate(text)['prediction'][0] Result: 1
#Using a review that has not beed stated Helpful on Amazon YET text=\u0026quot;Liked it\u0026quot; light_model.annotate(text)['prediction'][0] Result: 0
#Using a review that has not beed stated Helpful on Amazon YET text=\u0026quot;I tossed it in the trash. It smelled so bad.\u0026quot; light_model.annotate(text)['prediction'][0] Result: 0
Business Recommendations # Sellers can identify helpful reviews in advance and use the insights from reviews to optimize their selling strategies by endorsing helpful reviews and use them as part of marketing campaigns. ​
Sellers can filter helpful reviews and analyze at a massive scale and derive key insights to improve products and understand target customers.​
Amazon can identify unhelpful reviews, helping them better rank ​ their reviews to improve overall customer satisfaction
`}).add({id:15,href:"/docs/project3/business-use-cases/",title:"Business Use Cases",description:"Five Business Use Cases # Case 1: Tweet Sentimental Analysis Using TextBlob And NLP # NLP powered tweet analysis # Sentiment analysis is the task of determining the emotional value of a given expression in natural language. Using collected tweets data per brand, perform sentimental analysis using - TextBlob Textblob (NLP Api) sentiment analyzer returns two properties for a given input sentence: Polarity: -1 is negative, +1 is positive sentiment Subjectivity: Subjective sentences generally refer to personal opinion, emotion, or judgment.",content:`Five Business Use Cases # Case 1: Tweet Sentimental Analysis Using TextBlob And NLP # NLP powered tweet analysis # Sentiment analysis is the task of determining the emotional value of a given expression in natural language. Using collected tweets data per brand, perform sentimental analysis using - TextBlob Textblob (NLP Api) sentiment analyzer returns two properties for a given input sentence: Polarity: -1 is negative, +1 is positive sentiment Subjectivity: Subjective sentences generally refer to personal opinion, emotion, or judgment. Use polarity to assign word to each tweet: positive, negative and neutral Clients can get insights from sentimental analysis of customers’ tweets. This graph shows the relative perception of brands based on tweet sentiment analysis Again, tweets are processed through an NLP model and classified if they were either: Positive, Neutral or Negative in sentiment For example, GLAMGLOW has received a lot of flak and bad press KORRES on the other hand has generated a lot of positive buzz on Twitter Case 2.1: Key Performance Brand Level # This graph shows the relative perception of brands based on tweet sentiment analysis Again, tweets are processed through an NLP model and classified if they were either: Positive, Neutral or Negative in sentiment For example, GLAMGLOW has received a lot of flak and bad press KORRES on the other hand has generated a lot of positive buzz on Twitter Case 2.2: Key Performance Product Level # This graph shows the relative perception of brands based on tweet sentiment analysis Again, tweets are processed through an NLP model and classified if they were either: Positive, Neutral or Negative in sentiment For example, GLAMGLOW has received a lot of flak and bad press KORRES on the other hand has generated a lot of positive buzz on Twitter Case 3: User Journey On Demo App Built On Tkinter (Python GUI) # User opens the application User searches for the interested product Search query result is displayed with brand logo and product photo Shows the product\u0026rsquo;s ingredients and whether it contains harmful ingredients with a kind warning.​
Users can get comprehensive information on interested products’ ingredients and whether it contains any harmful substance
Business Case Convert users to purchasing products by becoming go-to place for searching cosmetics products Affiliate revenue is sizable with growing user base
Case 4: Competitor Lookup # Clients can get a comprehensive overview of competitor brands.
Used Tableau to create a Brand Dashboard Dashboard uses brand name as a filter so that the user can choose which brand to look into Dashboard contains information like brand description and their products Dashboard also shows the brand logo and its official website if present Case 5.1: Customer Demographic and Attribute Distribution # Clients can understand customers based on user review analysis. The brand\u0026rsquo;s main audience are between 18 to 34, although there are also customers from other age bands Most customers are lighter skinned Eye color distribution show that they have an area of improvement where they can release products that may match better with gray eyes Case 5.2: Average Product Rating Over Time # Some products might be seasonal. Product review started off strong, but tapers off with slower frequency of review and dropping average rating value Might suggest that product is not as good as expected or longer-term issues with the product that wasn\u0026rsquo;t immediately apparent (e.g., weather incompatibility) Case 5.3: Product for Specific Area of Concern # Review data can tell clients where users find the product most useful. “Purity Made Simple Cleanser” was made for acne problems in mind May also help with aging skin Not a good fit for customers who are looking to fix stretch marks or sun damage Brands can know consumers’ pain points and use insights to target marketing and improve formula to strengthen the product positioning Consumer can make better informed decision about their intended purchasing before paying Case 5.4: Top Products with Certain Attributes In Mind # Review data can tell clients all time favourites based on users’ attributes.
For undecided customers, they can first look up for items that are often used by others who share similar attributes For example, they may have a \u0026ldquo;combination\u0026rdquo; skin type, light skin tone, and blue eye color This insight can help them choose the right product in a specific category (e.g., foundation) Graph shows each product\u0026rsquo;s \u0026ldquo;love\u0026rdquo; rating (a measure of how many times customers \u0026ldquo;love\u0026rdquo; a product), and their numeric rating `}).add({id:16,href:"/docs/project2/models/",title:"Models",description:"Models # Our goal is to predict the sale price of a used car, which is a supervised regression problem. We pick our models base on two considerations, flexibility (accuracy) and interpretability.​ We value model accuracy over interpretability because:​ The industry we are in doesn’t require we provide explanation for the decision we make.​ Features of our used car dataset are easy to understand, thus making it easy for us to debug the model even without high model interpretability.",content:`Models # Our goal is to predict the sale price of a used car, which is a supervised regression problem. We pick our models base on two considerations, flexibility (accuracy) and interpretability.​ We value model accuracy over interpretability because:​ The industry we are in doesn’t require we provide explanation for the decision we make.​ Features of our used car dataset are easy to understand, thus making it easy for us to debug the model even without high model interpretability.​ Model Tradeoffs Our Model Selection # Linear Regression​
Support Vector Regression with linear kernel​
Decision Trees Ensemble method ​
Bagging Trees (Random Forest)​
Boosting Trees
Initial Model Selection: ​ # Linear Regression is not flexible enough to capture all the variance of the model​ SVR would be very slow to train. (SVR training time scale badly with large number of training sample)​ Ensemble Trees would be the best method as it is flexible and has decent interpretability​ ​ Our Approach # Train and tune all the models and compare the models’ accuracy​ Select the model with the best metric scores​ Our Metrics # R squared: the proportion of the variance explained by the model​ Root Mean Squared Error Mean Absolute Proportional Error `}).add({id:17,href:"/docs/project2/result/",title:"Result",description:`Model Results # Just like our expectation, Ensemble Tree methods, specifically XGBoost has the best overall performance. ​ This is not very surprising since ensemble method is known to:​
Have higher predictive accuracy, compared to the individual models.​ Be very useful when there is both linear and non-linear type of data in the dataset​ Linear Regression and Linear SVR, like expected, didn’t perform well. From the R squared score, we see that both models could not capture all the variance of our data.`,content:`Model Results # Just like our expectation, Ensemble Tree methods, specifically XGBoost has the best overall performance. ​ This is not very surprising since ensemble method is known to:​
Have higher predictive accuracy, compared to the individual models.​ Be very useful when there is both linear and non-linear type of data in the dataset​ Linear Regression and Linear SVR, like expected, didn’t perform well. From the R squared score, we see that both models could not capture all the variance of our data.​ XGBoost performs better than Random Forest.
Model R Squared Train RMSE Validation/ Test RMSE Test MAPE Linear Regression 0.68 5222.94 5272.66 0.49 Linear SVR 0.65 5613.04 5626.03 0.43 Random Forest 0.89 2971.65 3215.39 0.25 Gradient Boosting Machine 0.89 3111.86 3179.89 0.25 XGBoost 0.92 2400.81 2674.25 0.208 MAPE below 0.2 is considered a good score. We handled overfitting by comparing Train \u0026amp; Test RMSE.
Code Reference
Final Models
Feature Importance # MSRP, Odometer, Year, State are top important features. Key Findings # MSRP, odometer, and production year are proven to be top 3 strongest determinants of used car prices.​
Expected from initial EDA as we observed correlations​
States determine price range.​
Higher price variance as years go by.​
Some cars are not being sold as advertised (ex. Vintage cars may be lemons).​
Challenges/Areas of Improvement # Employ highly advanced NLP on textual data (description) excluding Ads, supplement​ the data with public reviews on each car, and apply topical modeling into our features. ​
Perform deeper research on car models with missing values and perform more​ thorough anomaly detection.​
We could integrate image detection algorithms to see whether car is described as it is and additionally use them as features for modelling (CNN Image Classification)
Recommendations # Proposed Business Application To Problems of Information Asymmetry: ​ # Craigslist or other platforms can present predictions (using the predictive model) of​ used cars so that buyers can get a sense of what is reasonable and have a base point ​ for comparison. ​
Craigslist should require sellers to fill in clearly defined forms for used cars so that​ \u0026lsquo;information asymmetry\u0026rsquo; can be mitigated. (Now, it is not mandatory. \u0026lsquo;Condition\u0026rsquo; criteria​ is also not clear, while this can be an important indicator.)​
Craigslist can also add exception criteria or specific section for vintage cars.​
For reputation and quality assurance purposes, used car companies can use the ​ predictions to target and filter out sellers prone to selling lemons prior to posting for​ sale.​​
Eventually, all these adjustments can be expected to improve the quality of used car​ listings in Craigslist, which in turn, can improve transaction success rate.​
​
`}).add({id:18,href:"/docs/project3/result/",title:"Result",description:"Lessons Learned # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie. Tweepy api gives only recent tweets) Heavy web scraping needs ways to bypass access restrictions (ie. Proper header and referrer required to access Sephora website) JavaScript is not rendered with only Python requests Data clean-up is indispensable, yet requires the most amount of time investment after data collection Using a balance of MySQL and NoSQL based on data types improves data handling and ease of producing analyses Expectations on data completeness should be based on data availability and must be managed early on GCP allows for synchronized working environment for team Recommendations # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie.",content:"Lessons Learned # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie. Tweepy api gives only recent tweets) Heavy web scraping needs ways to bypass access restrictions (ie. Proper header and referrer required to access Sephora website) JavaScript is not rendered with only Python requests Data clean-up is indispensable, yet requires the most amount of time investment after data collection Using a balance of MySQL and NoSQL based on data types improves data handling and ease of producing analyses Expectations on data completeness should be based on data availability and must be managed early on GCP allows for synchronized working environment for team Recommendations # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie. Tweepy api gives only recent tweets) Heavy web scraping needs ways to bypass access restrictions (ie. Proper header and referrer required to access Sephora website) JavaScript is not rendered with only Python requests Data clean-up is indispensable, yet requires the most amount of time investment after data collection Using a balance of MySQL and NoSQL based on data types improves data handling and ease of producing analyses Expectations on data completeness should be based on data availability and must be managed early on GCP allows for synchronized working environment for team "}).add({id:19,href:"/docs/project3/",title:"Brand \u0026 Product Perception",description:"Project3 Doks.",content:""}).add({id:20,href:"/docs/project1/",title:"NLP Classification",description:"Project1 Doks.",content:""}).add({id:21,href:"/docs/project2/",title:"Used Car Price Prediction",description:"Project2 Doks.",content:""}).add({id:22,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()