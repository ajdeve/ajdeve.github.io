<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Brand &amp; Product Perception on</title><link>https://ajdeve.github.io/docs/project3/</link><description>Recent content in Brand &amp; Product Perception on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 14 Mar 2022 08:48:45 +0000</lastBuildDate><atom:link href="https://ajdeve.github.io/docs/project3/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://ajdeve.github.io/docs/project3/introduction/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/introduction/</guid><description>Analysis On Beauty Industry # Our goal is to bring more meaningful and multi-dimensional data to users on cosmetic brands and products so that both brands and end-consumers can make informed decisions
Data Collection: utilize different tools such as Tweepy API, XHR requests to extract product, twitter, brands, reviews and ingredient data from various sources
Data Cleaning: utilize Python and OpenRefine to clean data and make them ready to import into MySQL</description></item><item><title>Data Profile</title><link>https://ajdeve.github.io/docs/project3/data-profile/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/data-profile/</guid><description>Data Infrastructure # Data Profile # SOURCE DESCRIPTION DATA SIZE FORMAT Product Kaggle Products listed on Sephora 12.4MB csv Brands Phantom Buster Information on cosmetic brands and company information 226KB json/csv Reviews BazaarVoice API User product reviews on Sephora 500MB json Twitter Tweepy API Users’ tweets with hashtag of brand name 7.3MB csv Ingredient California Safe Cosmetics Program Product DB Chemicals used in cosmetics that are known to be harmful 2.</description></item><item><title>Data Collection</title><link>https://ajdeve.github.io/docs/project3/data-collection/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/data-collection/</guid><description>XHR # XHR provided a better solution to scrape Sephora product reviews over BS4 &amp;amp; Selenium
Since scraping with both BS4 and Selenium did not work, we turned to XHR requests This turned to our advantage since the XHR API was generally cleaner, more complete product listing (website removed old products) and contains reviewer metadata
Tweepy API # Tweeter developer account created to use the free-tier api calls to scrape tweet data per brand (brand, tweet_id, tweet_date, follower_count, retweets, location, tweet_text) Iterate over brand names in brands table and append collected tweets to dataframe using Pandas For 186 brands, avg 110 tweets per brand were collected but number of tweets varied per brand, which could result in diminishing the value of analysis Phantom Buster # Phantom Buster was a great substitute to BS4 to scrape brand data.</description></item><item><title>Data Preparation</title><link>https://ajdeve.github.io/docs/project3/data-preparation/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/data-preparation/</guid><description>Data Cleanup # Remove NaN Remove Emojis from tweets REGEX to clean tweet text REGEX: Tweet text data includes emoji, hashtag, numbers, links and etc so REGEX is used to clean up each text
NLTK: NLTK tokenizing by word was used to extract more meaningful and logical words from cleaned tweet texts
Clustering On Location Data From Tweets # OpenRefine Tweets’ location is not structured so cleaning up the users’ various input on location required heavy clustering that involved:</description></item><item><title>Data Diagrams</title><link>https://ajdeve.github.io/docs/project3/data-diagrams/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/data-diagrams/</guid><description>OLTP: EER Diagram # There are 8 tables in the database After data collection and processing, import our data into MySQL Workbench and perform reverse enginnering to create the schema Define attributes with approriate data type Follow standard naming convention for tables and attributes Each table contains its unique key as primary key OLAP: Dimensional Diagram # Dimensional Model was developed based on the EER model constructed Used Snowflake Schema and normalized some dimension tables such as location table and product table “Reviews” is the fact table and there are 4 entities: customers, brands, product and date An extra entity, “dim_customers”, exists in dimensional diagram.</description></item><item><title>Business Use Cases</title><link>https://ajdeve.github.io/docs/project3/business-use-cases/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/business-use-cases/</guid><description>Five Business Use Cases # Case 1: Tweet Sentimental Analysis Using TextBlob And NLP # NLP powered tweet analysis # Sentiment analysis is the task of determining the emotional value of a given expression in natural language. Using collected tweets data per brand, perform sentimental analysis using - TextBlob Textblob (NLP Api) sentiment analyzer returns two properties for a given input sentence: Polarity: -1 is negative, +1 is positive sentiment Subjectivity: Subjective sentences generally refer to personal opinion, emotion, or judgment.</description></item><item><title>Result</title><link>https://ajdeve.github.io/docs/project3/result/</link><pubDate>Mon, 14 Mar 2022 08:48:45 +0000</pubDate><guid>https://ajdeve.github.io/docs/project3/result/</guid><description>Lessons Learned # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie. Tweepy api gives only recent tweets) Heavy web scraping needs ways to bypass access restrictions (ie. Proper header and referrer required to access Sephora website) JavaScript is not rendered with only Python requests Data clean-up is indispensable, yet requires the most amount of time investment after data collection Using a balance of MySQL and NoSQL based on data types improves data handling and ease of producing analyses Expectations on data completeness should be based on data availability and must be managed early on GCP allows for synchronized working environment for team Recommendations # Larger datasets require paid API calls so data costs should be considered more carefully in data planning process Research API restrictions in data collection is essential as expectation and reality may differ (ie.</description></item></channel></rss>